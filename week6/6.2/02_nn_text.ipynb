{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Νευρωνικά Δίκτυα και Κείμενο (Ελάχιστη Εισαγωγή)\n",
    "\n",
    "Υλικό προσαρμοσμένο από [σχετικό παράδειγμα της τεκμηρίωσης του TensorFlow](https://www.tensorflow.org/tutorials/keras/text_classification).\n",
    "\n",
    "---\n",
    "\n",
    "> Πάνος Λουρίδας, Αναπληρωτής Καθηγητής <br />\n",
    "> Τμήμα Διοικητικής Επιστήμης και Τεχνολογίας <br />\n",
    "> Οικονομικό Πανεπιστήμιο Αθηνών <br />\n",
    "> louridas@aueb.gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα χρησιμοποιήσουμε ένα σύνολο δεδομένων το οποίο περιέχει το κείμενο 50.000 κριτικών από το [Internet Movie Database (IMDb)](https://www.imdb.com/).\n",
    "\n",
    "* Αυτές είναι διαχωρισμένες σε 25.000 κριτικές για εκπαίδευση και 25.000 κριτικές για έλεγχο.\n",
    "\n",
    "* Σκοπός μας είναι να φτιάξουμε ένα μοντέλο το οποίο θα διαβάζει μία κριτική και θα μπορεί να αποφανθεί αν είναι θετική ή αρνητική.\n",
    "\n",
    "* Για περισσότερες πληροφορίες βλ. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics. Διαθέσιμο στο <https://aclanthology.org/P11-1015>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα πρέπει να αποσυμπιέσουμε το αρχείο `aclImdb.zip`.\n",
    "\n",
    "* Θα δημιουργηθεί ένας κατάλογος `aclImdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'aclImdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Τα δεδομένα περιέχονται σε δύο υποκαταλόγους, `train` και `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README', 'test', 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Για να δούμε τι υπάρχει στον κατάλογο `train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* O κατάλογος `pos` περιέχει θετικές κριτικές, κάθε μία σε ένα ξεχωριστό αρχείο.\n",
    "\n",
    "* Ομοίως, ο κατάλογος `neg` περιέχει αρνητικές κριτικές, κάθε μία σε ένα ξεχωριστό αρχείο.\n",
    "\n",
    "* Για να δούμε μία κριτική."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Για να πάρουμε τα δεδομένα μέσα στο TensorFlow θα χρησιμοποιήσουμε τη συνάρτηση `text_dataset_from_directory()`.\n",
    "\n",
    "* Αυτή προϋποθέτει ότι τα δεδομένα μας είναι τοποθετημένα σε καταλόγους, έναν για κάθε διακριτή κλάση, δηλαδή:\n",
    "\n",
    "  ```\n",
    "  main_directory/\n",
    "  ...class_a/\n",
    "  ......a_text_1.txt\n",
    "  ......a_text_2.txt\n",
    "  ...class_b/\n",
    "  ......b_text_1.txt\n",
    "  ......b_text_2.txt\n",
    "  ```\n",
    "  \n",
    "* Αυτό ακριβώς έχουμε ήδη."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα πάρουμε το υποσύνολο εκπαίδευσης.\n",
    "\n",
    "* Θα κρατήσουμε 20% για επικύρωση (πέρα από τα δεδομένα ελέγχου)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ας δούμε μερικές κριτικές και την κλάση στην οποία ανήκουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b'Many people here say that this show is for kids only. Hm, when I was a kid (approximately 7-9 years old) I watched this show first. It was disgusting for me. I talked with other kids about this and, sure, other shows and know what? This was the measure of disguise, whenever we wanted to emphasize something\\'s silliness (either on TV or anything else) we said \"Uh, just like Power Rangers\" and laughed. <br /><br />And before visiting this site I could not imagine that there actually are fans of MMPR. It was so strange for me that I decided to watch it again and try to understand why people like it. I did not enjoy that viewing. But it dawned upon me: maybe I have not enough imagination? It may be. However this argument is not sufficient for me to rate it more than 1 star.'\n",
      "Label 0\n",
      "Review b\"An uninteresting addition to the stalk 'n slash cycle which dominated the horror genre in the 1980's. This was filmed as Pranks but released as The Dorm That Dripped Blood which is an obvious steal from the 1970 horror anthology The House That Dripped Blood. Daphne Zuniga is the only recognisable face in the cast and this was her first horror movie (she has also appeared in The Initiation and The Fly II).\"\n",
      "Label 0\n",
      "Review b'***SPOILERS*** ***SPOILERS*** Released in 1956,and considered quite racy at the time, Douglas Sirk\\'s over the top candy colored melodrama is still a wonderful thing. The plot concerns the goings on in an oil rich dysfunctional Texas family that includes big brother Kyle, who is insecure, weak, wounded & very alcoholic, played by Robert Stack in a very touching & vulneable performance and his sluty sister Marylee played in an extreme manner by Dorothy Malone. Ms. Malone\\'s performance is telegraphed to us via her eyes, which she uses to show us her emotions, which mostly consist of lust (for Rock Hudson) and jealousy (for Lauren Bacall). Malone is the only actress I\\'ve ever seen in movies who enters a room eyes first. Now don\\'t get me wrong, her performance to say the least is an absolute hoot, and is one of the supreme camp acting jobs of the 1950\\'s. But it is also terrible, because as likeable and attractive as Malone is,she\\'s not a very good actress, and she\\'s not capable of subtly or shading. Her performace is of one note. She does get to do a wicked Mambo,and in a great montage, as unloving daddy played by the always good Robert Keith falls to his death climbing a staircase, Sirk mixes it up with an almost mad Malone doing a orgasmic dance as she undresses. Stack,(who should have won an Oscar) & Malone, (who won the award, but shouldn\\'t have) are the real stars of the film, the ones who set all the hysteria, both sexual & otherwise in motion, while the \"real stars\" of the film, Hudson & Bacall fade to grey & brown,which are the colors that they are mainly costumed in. Hudson who was a better actor then given credit for plays the childhood & best friend of Stack\\'s, and the stalked love interest of Malone\\'s who moans & groans over Rock through most of the film. But Hudson wants no part of her,and instead is in love with Bacall who is married to Stack. No one is very happy & no one is happy for very long. The Stack-Bacall marriage falls apart big time after a year, and Stack pretty much drinks himself into oblivion because he thinks he is sterile, and can\\'t give Bacall a baby to prove that he\\'s a man. Sirk who was a very intelligent man, and had a long & fascinating career both in films and theatre in Germany, ended his Hollywood career at Universal in the mid 1950\\'s with a series of intense vividly colored \"women\\'s movies\" or melodramas. Although they were mainly adapted from medicore or trashy source material,in Sirk\\'s hands they became masterpieces of the genre. Sirk had a wonderful sense of color & design which he brought to play in these films filling his wide screen spaces with characters who played out their emotional lives among weird color combinations & lighting, make believe shadows, and lots of mirroed reflections. In \"Written\" the characters are always peeking out of windows, listening at doors or sneaking around. So in the end, after much violence, an accidental murder, a miscarriage & more Sirk ends the movie with a final & startling scene of a \"reborn\" and reformed Malone in a man-tailored suit, sitting at a desk foundling a miniature oilwell.'\n",
      "Label 1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(0, 2+1):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Για να δούμε σε τι αντιστοιχούν οι δύο κλάσεις `0` και `1` μπορούμε να χρησιμοποιήσουμε την ιδιότητα  `class_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Αφού πήραμε το υποσύνολο εκπαίδευσης, θα πάρουμε τώρα το υποσύνολο επικύρωσης. \n",
    "\n",
    "* Προσέξτε ότι χρησιμοποιούμε τον ίδιο σπόρο (`seed`) για να είμαστε σίγουροι ότι δεν θα υπάρχουν επικαλύψεις μεταξύ των δεδομένων εκπαίδευσης και επικύρωσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Τέλος, παίρνουμε και τα δεδομένα ελέγχου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Όπως είδαμε προηγουμένως, οι κριτικές περιέχουν εκτός από κείμενο και την αλλαγή γραμμής σε HTML (`<br />`). \n",
    "\n",
    "* Εμείς θα τα αφαιρέσουμε αυτά.\n",
    "\n",
    "* Επίσης θα κάνουμε όλους τους χαρακτήρες πεζούς και θα αφαιρέσουμε σημεία στίξης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation),\n",
    "                                    '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Στη συνέχεια θα φτιάξουμε ένα στρώμα `TextVectorization`.\n",
    "\n",
    "* Αυτό το στρώμα θα κάνει τις παρακάτω προεργασίες:\n",
    "\n",
    "  * Θα καλέσει την `custom_standardization()`.\n",
    "\n",
    "  * Θα χωρίσει κάθε συμβολοσειρά που αντιστοιχεί σε μία κριτική σε επιμέρους λεκτικές μονάδες (tokens), χρησιμοποιώντας ως διαχωριστή χαρακτήρες κενών.\n",
    "  \n",
    "  * Θα αντιστοιχίσει σε κάθε μία από τις πιο συχνά εμφανιζόμενες λεκτικές μονάδες έναν ακέραιο αριθμό, δημιουργώντας ένα λεξικό μεγέθους 10.000.\n",
    "  \n",
    "  * Θα εξασφαλίσει ότι κάθε σειρά ακεραίων που θα προκύψει (που θα αναπαριστά κάθε κριτική) θα έχει το ίδιο μήκος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Καλούμε τη μέθοδο `adapt()` του `vectorize_layer` στο σύνολο εκπαίδευσης ώστε να κατασκευαστεί η αντιστοίχιση (λεξικό, vocabulary) μεταξύ μονάδων και ακεραίων. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Για να δούμε καλύτερα τι κάνει το `vectorize_layer`, θα γράψουμε μια βοηθητική συνάρτηση με την οποία θα μπορούμε να το καλούμε στα δεδομένα μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    # text is a tensor with shape (), we need to make it with shape (1,)\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b\"I'm never much for classic films. Movies like Patton, Going My Way, How Green was My Valley, The Godfather, Casablanca, Annie Hall, Gone with the Wind, Lawrence of Arabia, and Citizen Kane bore me. However, I would much rather watch any one of those films 3,469 times while being tied up on a chair than watch An American in Paris once in the most luxurious suite ever. If I did the latter, I'd probably be sleeping the entire time.<br /><br />The color art direction and the music didn't interest me, Gershwin or non-Gershwin. The dancing and the singing could help an insomniac fall to sleep. The dialogue doesn't match up to Singin' in the Rain. Basically, this movie is boring. The only other film that I fell asleep while watching was Butch Cassidy and the Sundance Kid. But you can't blame me. I only slept 5 minutes the night before.<br /><br />1 star/10 (Too bad we can't give zeroes.)\", shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[ 141,  110,   72,   15,  352,   94,   91,   38, 9276,  168,   54,\n",
      "          95,   87, 1504,   13,   54, 5019,    2, 3174, 7979, 4134, 2432,\n",
      "         812,   16,    2, 1910, 4376,    5,    1,    3, 3680, 3488, 2739,\n",
      "          69,  189,   10,   59,   72,  239,  103,   97,   28,    5,  143,\n",
      "          94,    1,  206,  132,  108, 3229,   56,   20,    4, 3288,   70,\n",
      "         103,   33,  312,    8, 1371,  272,    8,    2,   88,    1, 9968,\n",
      "         121,   45,   10,  117,    2, 1503,  450,  235,   26, 2570,    2,\n",
      "         417,   58,    2, 1391,  525,  448,    3,    2,  222,  152,  598,\n",
      "          69, 8672,   41,    1,    2, 1110,    3,    2, 1158,   98,  319,\n",
      "          33,    1,  759,    6, 1658,    2,  399,  144,  983,   56,    6,\n",
      "        9987,    8,    2, 2238,  658,   11,   17,    7,  346,    2,   61,\n",
      "          78,   19,   12,   10, 1464, 2308,  132,  146,   13, 6336, 5546,\n",
      "           3,    2, 7298,  554,   18,   22,  174, 1774,   69,   10,   61,\n",
      "        8187,  661,  226,    2,  313,  153,  473,    1,   99,   80,   71,\n",
      "         174,  193,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Όπως βλέπουμε, κάθε λεκτική μονάδα αναπαρίσταται από έναν ακέραιο αριθμό.\n",
    "\n",
    "* Ο αριθμός 0 χρησιμοποιείται για γέμισμα (padding), ώστε κάθε κριτική να έχει το ίδιο μήκος.\n",
    "\n",
    "* Ο αριθμός 1 χρησιμοποιείται για άγνωστες λέξεις, δηλαδή λέξεις που είναι εκτός των 10.000 συχνότερων, τις οποίες κρατάει το λεξικό μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --->  \n",
      "1 --->  [UNK]\n",
      "1287 --->  silent\n",
      "9999 --->  rushes\n",
      "Vocabulary size:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"0 ---> \", vectorize_layer.get_vocabulary()[0])\n",
    "print(\"1 ---> \",vectorize_layer.get_vocabulary()[1])\n",
    "print(\"1287 ---> \", vectorize_layer.get_vocabulary()[1287])\n",
    "print(\"9999 ---> \",vectorize_layer.get_vocabulary()[9999])\n",
    "print(\"Vocabulary size: \", vectorize_layer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Για να βελτιώσουμε την ταχύτητα, θα χρησιμοποιήσουμε τις μεθόδους `cache()` και `prefetch()`. \n",
    "\n",
    "* Με τη μέθοδο `cache()`, τα δεδομένα την πρώτη φορά που διαβάζονται μπορούν να αποθηκευτούν στη μνήμη.\n",
    "\n",
    "* Με τη μέθοδο `prefetch()`, τα δεδομένα τροφοδούνται στο νευρωνικό δίκτυο καθώς το δίκτυο επεξεργάζεται ήδη τα προηγούμενα δεδομένα, ώστε να μην χάνεται χρόνος για την τροφοδοσία (αφού μπορεί να γίνεται παράλληλα με την επεξεργασία)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "raw_test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Προχωράμε στην κατασκευή του μοντέλου μας.\n",
    "\n",
    "* Το πρώτο επίπεδο που βάζουμε στο μοντέλο μας είναι το `vectorize_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Στη συνέχεια θα προσθέσουμε ένα *στρώμα ενσωμάτωσης* (embedding layer).\n",
    "\n",
    "* Το στρώμα αυτό μετατρέπει τον ακέραιο αριθμό που αναπαριστά κάθε λεκτική μονάδα σε ένα *διάνυσμα* 16 διαστάσεων (δική μας επιλογή).\n",
    "\n",
    "* Επομένως περνάμε από μια αναπαράσταση «λέξη-αριθμός» σε μία αναπαράσταση «λέξη-διάνυσμα». \n",
    "\n",
    "* Η διανυσματική αυτή αναπαράσταση, η ενσωμάτωση, θα εκφράζει κατά κάποιον τρόπο το νόημα της κάθε λεκτικής μονάδας.\n",
    "\n",
    "* Πώς προκύπτει η διανυσματική αναπαράσταση κάθε λέξης; Θα τη μάθει το δίκτυο!\n",
    "\n",
    "* Η είσοδος του στρώματος ενσωμάτωσης είναι μια ακολουθία ακεραίων αριθμών, μήκους 250.\n",
    "\n",
    "* Η έξοδος του στρώματος θα είναι πλέον ένας πίνακας διαστάσεων $250 \\times 16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model.add(layers.Embedding(max_features, embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Aκολουθεί ένα στρώμα dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Κάθε κριτική αναπαραστάται με έναν πίνακα διαστάσεων $250 \\times 16$.\n",
    "\n",
    "* Από αυτήν θα παράξουμε ένα διάνυσμα με 16 διαστάσεις.\n",
    "\n",
    "* Αυτό θα το κάνουμε με ένα στρώμα `GlobalAveragePooling1D`.\n",
    "\n",
    "* Από τα 250 διανύσματα 16 διαστάσεων θα πάρουμε το μέσο όρο τους.\n",
    "\n",
    "* Διαισθητικά, αυτή θα είναι η διανυσματική αναπαράσταση του «μέσου όρου» του νοήματος των λεκτικών μονάδων της κάθε κριτικής.\n",
    "\n",
    "* Ακόμα πιο διαισθητικά, αυτή θα αντιστοιχεί το νόημα (σε μία ιδεατή λέξη) που συνοψίζει όλη την κριτική."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Τέλος, θα προσθέσουμε ένα πυκνά συνδεμένο νευρώνα στο τελευταίο στρώμα ώστε να κάνουμε την ταξινόμηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Να δούμε συνοπτικά τι έχουμε:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m250\u001b[0m)                    │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Όπως συνήθως, ορίζουμε τον βελτιστοποιητή, απώλεια, και μετρική. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Προχωράμε στην εκπαίδευση για δέκα εποχές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - binary_accuracy: 0.5872 - loss: 0.6793 - val_binary_accuracy: 0.7556 - val_loss: 0.6082\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - binary_accuracy: 0.7614 - loss: 0.5752 - val_binary_accuracy: 0.8188 - val_loss: 0.4954\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - binary_accuracy: 0.8242 - loss: 0.4660 - val_binary_accuracy: 0.8364 - val_loss: 0.4252\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.8517 - loss: 0.3948 - val_binary_accuracy: 0.8426 - val_loss: 0.3867\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.8695 - loss: 0.3485 - val_binary_accuracy: 0.8506 - val_loss: 0.3597\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.8827 - loss: 0.3169 - val_binary_accuracy: 0.8572 - val_loss: 0.3391\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.8917 - loss: 0.2927 - val_binary_accuracy: 0.8564 - val_loss: 0.3319\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.8995 - loss: 0.2720 - val_binary_accuracy: 0.8584 - val_loss: 0.3238\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.9047 - loss: 0.2538 - val_binary_accuracy: 0.8586 - val_loss: 0.3181\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - binary_accuracy: 0.9118 - loss: 0.2398 - val_binary_accuracy: 0.8590 - val_loss: 0.3145\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    raw_train_ds,\n",
    "    validation_data=raw_val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Αφού εκπαιδεύσουμε, μπορούμε να δούμε την επίδοση στα δεδομένα ελέγχου.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - binary_accuracy: 0.8534 - loss: 0.3323\n",
      "Loss:  0.33558571338653564\n",
      "Accuracy:  0.8528000116348267\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(raw_test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ας επανέλθουμε λίγο στη διανυσματική αναπαράσταση των λέξεων στο μοντέλο μας.\n",
    "\n",
    "* Τα διανύσματα για την κάθε λέξη είναι τα βάρη του στρώματος `Embedding`.\n",
    "\n",
    "* Το στρώμα αυτό έχει διαστάσεις `(vocabulary_size, embedding_dim)`.\n",
    "\n",
    "* Το νευρωνικό δίκτυο έμαθε τα βάρη, δηλαδή έμαθε τις διανυσματικές αναπαραστάσεις των λέξεων κατά την εκπαίδευση για την ταξινόμηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = model.layers[1]\n",
    "weights = embedding.get_weights()[0]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Επομένως από εκεί που είχαμε ξεκινήσει με την αναπαράσταση των λέξεων μέσω ακεραίων, τελικά αναπαριστούμε κάθε λέξη ως ένα σημείο σε έναν χώρο 16 διαστάσεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 [-0.0185614   0.0194432   0.03007898  0.02798059  0.01007916 -0.01115976\n",
      " -0.00890433 -0.01762084  0.00939872 -0.01185121 -0.15861313 -0.00091788\n",
      "  0.04795087  0.00974149  0.0148848   0.05098724]\n",
      "[UNK] 1 [ 0.00698835 -0.03073382 -0.02051858  0.13724235 -0.01101787 -0.12969722\n",
      "  0.03107361 -0.14283228  0.09152833  0.02735772 -0.02866978 -0.00552547\n",
      " -0.00997996 -0.0058208   0.03323147  0.11267732]\n",
      "the 2 [-0.06680274  0.06308053  0.02947343  0.0363104   0.11789925  0.16441283\n",
      "  0.03301602  0.09730844 -0.00126866 -0.10495811  0.0834692  -0.02802717\n",
      " -0.01584262 -0.04932199 -0.05790106 -0.11074435]\n",
      "and 3 [-0.1790187   0.22008419  0.13669398 -0.11167633  0.22184016  0.15032476\n",
      "  0.17866898  0.12094135 -0.27376938 -0.34408668  0.30223772 -0.13629651\n",
      " -0.1079473  -0.17269143 -0.1560232  -0.14057891]\n",
      "a 4 [-0.065204    0.18128988  0.08169492 -0.16755843 -0.04724656  0.02663736\n",
      "  0.00918333  0.10471648 -0.10845941 -0.10390326  0.08115645 -0.07032047\n",
      " -0.06874675 -0.10143736 -0.01967762 -0.07667676]\n",
      "of 5 [ 0.08184887 -0.01114134 -0.0682266   0.1220971   0.04658734 -0.13318901\n",
      " -0.06867569 -0.06494904  0.0341773   0.06149873 -0.05047824  0.10429639\n",
      "  0.04011074  0.16668618  0.19761163  0.16152593]\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, 5+1):\n",
    "    word = vectorize_layer.get_vocabulary()[num]\n",
    "    vec = weights[num]\n",
    "    print(word, num, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Η χρήση διανυσματικών αναπαραστάσεων λέξεων είναι η βάση στα νευρωνικά δίκτυα που χειρίζονται γλώσσα.\n",
    "\n",
    "* Εμείς στο απλό παράδειγμά μας χρησιμοποιήσαμε ένα μικρό σύνολο δεδομένων για την εκμάθηση των διανυσμάτων.\n",
    "\n",
    "* Στην πράξη, υπάρχουν διαθέσιμες διανυσματικές αναπαραστάσεις λέξεων που έχουν προκύψει από εκπαίδευση σε τεράστια σώματα κειμένου."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (Data Science)",
   "language": "python",
   "name": "data_science_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
